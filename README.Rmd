---
title: "README"
output: 
  github_document:
    pandoc_args: ["--wrap=none"]
always_allow_html: true
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)

```

```{r,warning = FALSE, message = FALSE, echo=FALSE, include=FALSE}
# Load required libraries
# List of libraries to check, install (if missing), and load
libraries <- c(
  "knitr", "dplyr", "ggplot2", "tidyr", "multcomp", 
  "kableExtra", "FSA", "MVN", "lavaan", "semPlot", 
  "psych", "boot", "markdown","moments","tidyverse","gghalves"
)

# Function to check, install, and load libraries
check_and_load <- function(lib) {
  if (!require(lib, character.only = TRUE)) {
    install.packages(lib, dependencies = TRUE)
    library(lib, character.only = TRUE)
  }
}

# Apply the function to all libraries
lapply(libraries, check_and_load)

set.seed(123)  # For reproducibility


```


Download that from GitHub Repo: https://github.com/

```{r}
# Define the GitHub raw CSV URL
#github_url <- "https://raw.githubusercontent.com//Hexad-/refs/heads/main/Hexad_12_Spanish_data.csv"

# Define a local file path to save the downloaded file
#local_file <- tempfile(fileext = ".csv")

# Download the file
#download.file(github_url, local_file)

# Load the CSV into a data frame
#data <- read.csv(local_file,row.names = 1)
data<-read.csv("Hexad_12_Spanish_data.csv")
#remove 1st non-value columns


# Display the first few rows of the data frame
head(data)
```
## General Statistics


#### Number of Participants
Each row represents a data point from a single participant. Total number of participants
```{r}
print(nrow(data))
```

#### Completion Time
```{r}
# Ensure Completion.Time is numeric
data$Completion.Time <- as.numeric(data$Completion.Time)

# Compute and create summary table
summary_table <- data.frame(
  Statistic = c("Average", "Median", "Min", "Max", "Standard Deviation", "Non-Empty Rows"),
  Value = c(
   round(mean(data$Completion.Time, na.rm = TRUE),2),
    round(median(data$Completion.Time, na.rm = TRUE),2),
    round(min(data$Completion.Time, na.rm = TRUE),2),
    round(max(data$Completion.Time, na.rm = TRUE),2),
    round(sd(data$Completion.Time, na.rm = TRUE),2),
    round(sum(!is.na(data$Completion.Time)),0)
  )
)

# Display the table
kable(summary_table, caption = "Summary Statistics for Completion Time")
```


#### Age
```{r}
# Ensure Completion.Time is numeric
data$Age <- as.numeric(data$Age)

# Compute and create summary table
summary_table <- data.frame(
  Statistic = c("Average", "Median", "Min", "Max", "Standard Deviation", "Non-Empty Rows"),
  Value = c(
   round(mean(data$Age, na.rm = TRUE),2),
    round(median(data$Age, na.rm = TRUE),2),
    round(min(data$Age, na.rm = TRUE),2),
    round(max(data$Age, na.rm = TRUE),2),
    round(sd(data$Age, na.rm = TRUE),2),
    round(sum(!is.na(data$Age)),0)
  )
)

# Display the table
kable(summary_table, caption = "Summary Statistics for Age")
```


#### Gender
```{r}
# Encode Gender as a factor with specific levels
gender_labels <- c("female", "male", "other")
data$Gender <- factor(data$Gender, levels = gender_labels)

# Compute summary statistics for Gender
gender_counts <- table(data$Gender)
total_count <- sum(gender_counts)
gender_percentages <- round((gender_counts / total_count) * 100, 2)

# Create a summary table
gender_summary_table <- data.frame(
  Gender = c(names(gender_counts), "Total"),
  Count = c(as.numeric(gender_counts), total_count),
  Percentage = c(paste0(gender_percentages, "%"), "100%")
)

# Display the table
kable(gender_summary_table, caption = "Summary of Gender Distribution with Totals")

```


#### Education Level
```{r}
# Encode Education Level as a factor with specific levels
educationlevel_labels <- c(
  "High School diploma",
  "Technical or vocational training",
  "Associate's degree",
  "Bachelor's degree",
  "Master's degree",
  "Professional degree",
  "Doctorate"
)

data$Education <- factor(data$Education, levels = educationlevel_labels)

# Compute counts and percentages
education_counts <- table(data$Education)
total_count <- sum(education_counts)
education_percentages <- round((education_counts / total_count) * 100, 2)

# Create a summary table
education_summary_table <- data.frame(
  EducationLevel = c(names(education_counts), "Total"),
  Count = c(as.numeric(education_counts), total_count),
  Percentage = c(paste0(education_percentages, "%"), "100%")
)

# Display the summary table
kable(education_summary_table, caption = "Summary of Education Level Distribution with Totals")
```


#### Playing Days
```{r}
# Ensure Completion.Time is numeric
data$Freq.Games <- as.numeric(data$Freq.Games)

# Compute and create summary table
summary_table <- data.frame(
  Statistic = c("Average", "Median", "Min", "Max", "Standard Deviation", "Non-Empty Rows"),
  Value = c(
   round(mean(data$Freq.Games, na.rm = TRUE),2),
    round(median(data$Freq.Games, na.rm = TRUE),2),
    round(min(data$Freq.Games, na.rm = TRUE),2),
    round(max(data$Freq.Games, na.rm = TRUE),2),
    round(sd(data$Freq.Games, na.rm = TRUE),2),
    round(sum(!is.na(data$Freq.Games)),0)
  )
)

# Display the table
kable(summary_table, caption = "Summary Statistics for Frequency of Play")
```

#### Country of Origin

```{r}
# Filter the data to exclude empty Country values
filtered_data <- data$Country[data$Country != ""]

# Compute counts and percentages
country_counts <- sort(table(filtered_data), decreasing = TRUE)
total_count <- sum(country_counts)
country_percentages <- round((country_counts / total_count) * 100, 2)

# Create a summary table with "Total" as the first row
summary_table <- data.frame(
  Country = c("Total", names(country_counts)),
  Count = c(total_count, as.numeric(country_counts)),
  Percentage = c("100%", paste0(country_percentages, "%"))
)

# Display the summary table
kable(summary_table, caption = "Summary of Country Distribution with Totals at the Top")

```



#### Hexad type  Distribution


```{r}


# Subset data to include the first six columns
subdata <- data[, c(1:6)]

# Specify the columns of interest
columns_of_interest <- c("Philanthropist", "Socializer", "Free Spirit", "Achiever", "Player", "Disruptor")

# Rename columns
colnames(subdata) <- columns_of_interest

# Calculate summary statistics and round to 2 decimal places
summary_stats <- data.frame(
  Column   = columns_of_interest,
  Mean     = round(sapply(subdata[columns_of_interest], mean,      na.rm = TRUE), 2),
  Median   = round(sapply(subdata[columns_of_interest], median,    na.rm = TRUE), 2),
  Min      = round(sapply(subdata[columns_of_interest], min,       na.rm = TRUE), 2),
  Max      = round(sapply(subdata[columns_of_interest], max,       na.rm = TRUE), 2),
  SD       = round(sapply(subdata[columns_of_interest], sd,        na.rm = TRUE), 2),
  Skewness = round(sapply(subdata[columns_of_interest], skewness,  na.rm = TRUE), 2),
  Kurtosis = round(sapply(subdata[columns_of_interest], kurtosis,  na.rm = TRUE), 2)
)

# Sort by descending Mean
summary_stats <- summary_stats[order(-summary_stats$Mean), ]

# Print the table nicely with knitr::kable
kable(summary_stats, 
      caption = "Summary Statistics of Hexad-12 determinants", 
      digits  = 2)


```



```{r}

library(tidyverse)
library(gghalves)
library(dplyr)
# ---- reshape to long form ---------------------------------------------------
hexad_long <- data %>% 
  dplyr::select(
    Philanthropist = 'philanthropist',
    Socializer     = 'socializer',
    `Free Spirit`  = 'freeSpirit',
    Achiever       = 'achiever',
    Player         = 'player',
    Disruptor      = 'disruptor'
  ) %>% 
  pivot_longer(
    cols      = everything(),
    names_to  = "Determinant",
    values_to = "Score"
  ) %>% 
  mutate(
    Determinant = fct_reorder(Determinant, Score, .fun = mean, .desc = TRUE)
  )

# ---- full violin + slim boxplot + jittered points --------------------------
ggplot(hexad_long, aes(x = Determinant, y = Score, fill = Determinant)) +
  # wider violin
  geom_violin(trim = FALSE, width = 1.0, alpha = 0.6) +
  # ultra-thin box in the center
  geom_boxplot(width = 0.08, fill = "white", outlier.shape = NA) +
  # jittered raw points
  geom_jitter(aes(color = Determinant+1),
              width = 0.1, height = 0.1,
              size = 1, alpha = 0.2,
              show.legend = FALSE) +
  scale_y_continuous(limits = c(1, 14), breaks = seq(2, 14, 2)) +
  guides(fill = FALSE) +
  labs(
    title = "Distribution of Hexad Motivational Determinants",
    x     = NULL,
    y     = "Score (0–14)"
  ) +
  theme_minimal(base_size = 12) +
  theme(panel.grid.major.x = element_blank())


```


## Hexad Player type Distributions Analysis By Country (top 10)

#### Distribution by Country (Top 10)

```{r}
#Shorten Countries Name for easy of tables
data[data$Country=="Bolivia (Estado Plurinacional de)", 'Country' ]<-"Bolivia"
data[data$Country=="Venezuela (República Bolivariana de)", 'Country' ]<-"Venezuela"
data[data$Country==" Estados Unidos de América", 'Country' ]<-"USA"
data$Country<-as.factor(data$Country)

# Filter out rows where 'Country' is missing
hexad_data_clean <- data %>% filter(Country != "")

# Identify the top 5, top 10, and top 20 most common countries
top_countries <- hexad_data_clean %>%
  count(Country, sort = TRUE)

top_10_countries <- top_countries %>% slice(1:10) %>% pull(Country)

# Filter data for these top countries
hexad_top10 <- hexad_data_clean %>% filter(Country %in% top_10_countries)


```

```{r}


# Calculate and round summary statistics by Country for each player type
hexad_summary <- hexad_top10 %>%
  group_by(Country) %>%
  summarise(
    sample_size = n(),
    philanthropist_mean = round(mean(philanthropist, na.rm = TRUE), 2),
    philanthropist_median = round(median(philanthropist, na.rm = TRUE), 2),
    philanthropist_min = round(min(philanthropist, na.rm = TRUE), 2),
    philanthropist_max = round(max(philanthropist, na.rm = TRUE), 2),
    philanthropist_sd = round(sd(philanthropist, na.rm = TRUE), 2),
    
    socializer_mean = round(mean(socializer, na.rm = TRUE), 2),
    socializer_median = round(median(socializer, na.rm = TRUE), 2),
    socializer_min = round(min(socializer, na.rm = TRUE), 2),
    socializer_max = round(max(socializer, na.rm = TRUE), 2),
    socializer_sd = round(sd(socializer, na.rm = TRUE), 2),
    
    freeSpirit_mean = round(mean(freeSpirit, na.rm = TRUE), 2),
    freeSpirit_median = round(median(freeSpirit, na.rm = TRUE), 2),
    freeSpirit_min = round(min(freeSpirit, na.rm = TRUE), 2),
    freeSpirit_max = round(max(freeSpirit, na.rm = TRUE), 2),
    freeSpirit_sd = round(sd(freeSpirit, na.rm = TRUE), 2),
    
    achiever_mean = round(mean(achiever, na.rm = TRUE), 2),
    achiever_median = round(median(achiever, na.rm = TRUE), 2),
    achiever_min = round(min(achiever, na.rm = TRUE), 2),
    achiever_max = round(max(achiever, na.rm = TRUE), 2),
    achiever_sd = round(sd(achiever, na.rm = TRUE), 2),
    
    player_mean = round(mean(player, na.rm = TRUE), 2),
    player_median = round(median(player, na.rm = TRUE), 2),
    player_min = round(min(player, na.rm = TRUE), 2),
    player_max = round(max(player, na.rm = TRUE), 2),
    player_sd = round(sd(player, na.rm = TRUE), 2),
    
    disruptor_mean = round(mean(disruptor, na.rm = TRUE), 2),
    disruptor_median = round(median(disruptor, na.rm = TRUE), 2),
    disruptor_min = round(min(disruptor, na.rm = TRUE), 2),
    disruptor_max = round(max(disruptor, na.rm = TRUE), 2),
    disruptor_sd = round(sd(disruptor, na.rm = TRUE), 2)
  ) %>%
  arrange(desc(sample_size)) # Sort by sample size

# Transpose the table
hexad_summary_t <- hexad_summary %>%
  pivot_longer(cols = -Country, names_to = "Statistic", values_to = "Value") %>%
  pivot_wider(names_from = Country, values_from = Value)

# Generate an RMarkdown table
hexad_summary_t %>%
  kbl(caption = "Transposed Summary Statistics for Hexad Player Types by Country") %>%
  kable_styling(full_width = TRUE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```



#### Normality tests
```{r}


# Initialize an empty data frame to store the results
shapiro_results <- data.frame(
  Row = c("Philanthropist", "Socializer", "Free Spirit", "Achiever", "Player", "Disruptor"),
  Shapiro_Wilk = numeric(6),
  P_value = numeric(6),
  stringsAsFactors = FALSE
)

# List of player types
player_types <- c("philanthropist", "socializer", "freeSpirit", "achiever", "player", "disruptor")

# Loop through each player type and perform the Shapiro-Wilk test
for (i in seq_along(player_types)) {
  type <- player_types[i]
  
  # Perform ANOVA to get residuals
  anova_5 <- aov(as.formula(paste(type, "~ Country")), data = hexad_top10)
  
  # Perform Shapiro-Wilk Test
  shapiro_test <- shapiro.test(residuals(anova_5))
  
  # Save results
  shapiro_results$Shapiro_Wilk[i] <- round(shapiro_test$statistic, 2)
  shapiro_results$P_value[i] <- shapiro_test$p.value
}

# Rename columns to match desired output
colnames(shapiro_results) <- c("Row", "Shapiro-Wilk", "p-value")

# Create and display the RMarkdown table
shapiro_results %>%
  kbl(caption = "Shapiro-Wilk Normality Test for Player Types by Country") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```

#### Non-parametric ANOVA tests
```{r}

# Initialize an empty data frame to store the results
results <- data.frame(
  Row = c("Philanthropist", "Socializer", "Free Spirit", "Achiever", "Player", "Disruptor"),
  Kruskal_Wallis_Chi_Sq = numeric(6),
  Degrees_of_Freedom = numeric(6),
  p_value = numeric(6),
  stringsAsFactors = FALSE
)

# List of player types
player_types <- c("philanthropist", "socializer", "freeSpirit", "achiever", "player", "disruptor")

# Loop through each player type and perform the Kruskal-Wallis test
for (i in seq_along(player_types)) {
  type <- player_types[i]
  
  # Perform Kruskal-Wallis Test
  kw_test <- kruskal.test(as.formula(paste(type, "~ Country")), data = hexad_top10)
  
  # Save results
  results$Kruskal_Wallis_Chi_Sq[i] <- round(kw_test$statistic, 2)
  results$Degrees_of_Freedom[i] <- kw_test$parameter
  results$p_value[i] <- round(kw_test$p.value, 3)
}

# Rename columns to match desired output
colnames(results) <- c(
  "Row",
  "Kruskal-Wallis χ^2",
  "Degrees of Freedom",
  "p-value"
)

# Create and display the RMarkdown table
results %>%
  kbl(caption = "Kruskal-Wallis Test Results for Player Types by Country") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```

#### Dunn test for multiple comparison for Achiever

```{r}

# Perform Dunn Test with Bonferroni adjustment
posthoc_test <- dunnTest(achiever ~ Country, data = hexad_top10, method = "bonferroni")

# Extract and round results
posthoc_results <- posthoc_test$res %>%
  mutate(across(where(is.numeric), ~ round(.x, 3))) # Round all numeric columns to 3 decimal places

# Format results as a nice RMarkdown table
posthoc_results %>%
  kbl(caption = "Post Hoc Dunn Test Results for Achiever by Country (Bonferroni Adjustment)") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```


## Confirmatory Factor Analysis

#### Mardia’s multivariate normality test 
```{r}

# Perform Mardia's test for multivariate normality
mardia_test <- mvn(data = data[, 7:18], mvnTest = "mardia")
mardia_test_N<-mardia_test$multivariateNormality
# Extract and prepare the results for the table
mardia_results <- data.frame(
  Test = c("Skewness", "Kurtosis"),
  Statistic = c(
    (mardia_test_N[1,2]),
    (mardia_test_N[2,2])
    
  ),
  P_Value = c(
  (mardia_test_N[1,3]),
    (mardia_test_N[2,3])
    
  )
)

# Create and display the RMarkdown table
mardia_results %>%
  kbl(caption = "Mardia's Test for Multivariate Normality") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

#### Weighted Least Squares Mean and Variance Adjusted   CFA
```{r}
# Specify the CFA model
cfa_model <- '
  Achiever =~ A2 + A4
  Disruptor =~ D3 + D4
  FreeSpirit =~ F1 + F3
  Philanthropist =~ P1 + P4
  Player =~ R2 + R4
  Socializer =~ S2 + S4
'

# Fit the CFA model
fit <- cfa(cfa_model, data = data,std.lv=TRUE, auto.efa = TRUE, estimator = "WLSMV")
 #
# Global Fit Metrics
results<-summary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)




# install.packages("kableExtra")  # if you haven’t already

kable(
  results$pe[1:12, c(2:8, 10)],
  caption = "Estimated factor loading for Hexad-12 items",
  digits  = 3,
  format = "html"
) %>%
  kable_styling(full_width = TRUE) %>%
  column_spec(1:ncol(results$pe[1:12, c(2:8, 10)]), width = "2cm")



```



###  CFA  metrics with confident interval
```{r}

reliability_metrics <- semTools::reliability(fit)
CR<-reliability_metrics[2,]
AVE<-reliability_metrics[5,]
  

  # Extract relevant fit measures
  fit_measures_final <- fitMeasures(fit,standardized = TRUE, rsquare = TRUE, c("chisq.scaled", "srmr", "tli.scaled", "cfi.scaled", "rmsea.scaled"))

  fit_measures_final<-c(fit_measures_final, CR, AVE)
  

# Function to take a bootstrap sample and perform CFA, returning CR, Omega, and AVE
bootstrap_cfa_cr_omega_ave <- function(data, cfa_model) {
  # Generate a bootstrap sample (with replacement)
  bootstrap_sample <- data[sample(1:nrow(data), replace = TRUE), ]
  

# Fit the CFA model
fit <- cfa(cfa_model, data = bootstrap_sample,std.lv=TRUE, auto.efa = TRUE, estimator = "WLSMV")

 # Calculate reliability metrics (including Composite Reliability)
reliability_metrics <- semTools::reliability(fit)
  CR<-reliability_metrics[2,]
 AVE<-reliability_metrics[5,]
  

  # Extract relevant fit measures
  fit_measures <- fitMeasures(fit, c("chisq.scaled", "srmr", "tli.scaled", "cfi.scaled", "rmsea.scaled"))

  fit_measures<-c(fit_measures, CR, AVE)
 
  return(fit_measures)
}

# Perform bootstrap sampling
set.seed(123)  # For reproducibility
num_bootstrap <- 10000  # Number of bootstrap samples8u-p0

# Initialize matrix to store CR, Omega, and AVE
fit_metrics <- matrix(NA, nrow = num_bootstrap, ncol = 17)
# List of columns to calculate confidence intervals for
columns_to_ci <- c("chisq.scaled", "srmr", "tli.scaled", "cfi.scaled", "rmsea.scaled", 
                   "CR.Achiever", "CR.Disruptor", "CR.FreeSpirit", "CR.Philanthropist", 
                   "CR.Player", "CR.Socializer", 
                   "AVE.Achiever", "AVE.Disruptor", "AVE.FreeSpirit", 
                   "AVE.Philanthropist", "AVE.Player", "AVE.Socializer")
colnames(fit_metrics) <- columns_to_ci

# Bootstrap loop
for (i in 1:num_bootstrap) {
  fit_metrics[i, ] <- bootstrap_cfa_cr_omega_ave(data, cfa_model)
}


  names(fit_measures_final) <- columns_to_ci


# Initialize an empty data frame to store the results
results_table <- data.frame(
  Metric = columns_to_ci,
  Estimate = numeric(length(columns_to_ci)),
  CI_Lower = numeric(length(columns_to_ci)),
  CI_Upper = numeric(length(columns_to_ci)),
  stringsAsFactors = FALSE
)

# Populate the results table with confidence intervals and estimates
for (i in seq_along(columns_to_ci)) {
  col <- columns_to_ci[i]
  ci <- quantile(fit_metrics[, col], probs = c(0.025, 0.975))
  results_table$Estimate[i] <- round(fit_measures_final[col], 3)  # Add the estimate and round
  results_table$CI_Lower[i] <- round(ci[1], 3)  # Round lower CI
  results_table$CI_Upper[i] <- round(ci[2], 3)  # Round upper CI
}

# Correctly print the table in a nicely formatted way
results_table %>%
  kbl(caption = "CFA  metrics with confident interval", format = "markdown",digits  = 2) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

#### CFA Standardized residuals
```{r}
# Local Fit: Residuals  Standardized residuals
standardized_residuals <- residuals(fit, type = "standardized")


kable(standardized_residuals, caption = "Standardized Residuals for the Hexad-12 items from the CFA",digits  = 3)

```

#### CFA Correlation residuals
```{r}
#Correlation residuals


kable(residuals(fit, type = "cor"), caption = "Correlation Residuals for the Hexad-12 items from the CFA",digits  = 3)

```


## Internal Reliability

```{r}
# Load necessary libraries
library(psych)  # For Cronbach's alpha and McDonald's omega
library(boot)   # For bootstrapping confidence intervals

# Define pairs of columns for which to calculate reliability metrics
pairs <- list(
  c("A2", "A4"),
  c("D3", "D4"),
  c("F1", "F3"),
  c("P1", "P4"),
  c("R2", "R4"),
  c("S2", "S4")
)


merged_hexad<-data
# Cronbach's alpha function
cronbach_alpha_fn <- function(data, indices, item1, item2) {
  samp <- data[indices, c(item1, item2)]
  psych::alpha(samp)$total$raw_alpha
}

# Spearman-Brown function
spearman_brown_fn <- function(data, indices, item1, item2) {
  samp <- data[indices, ]
  r <- cor(samp[[item1]], samp[[item2]], method = "pearson", use = "pairwise.complete.obs")
  2 * r / (1 + r)
}

# McDonald's omega function
mcdonald_omega_fn <- function(data, indices, item1, item2) {
  samp <- data[indices, c(item1, item2)]
  # fit 1-factor model and extract total omega
  psych::omega(samp, nfactors = 1, plot = FALSE)$omega.tot
}

# Prepare results data frame
results <- data.frame(
  Pair                      = character(),
  Alpha_Estimate            = numeric(), Alpha_CI_Lower = numeric(), Alpha_CI_Upper = numeric(),
  Spearman_Brown_Estimate   = numeric(), SB_CI_Lower    = numeric(), SB_CI_Upper    = numeric(),
  Omega_Estimate            = numeric(), Omega_CI_Lower = numeric(), Omega_CI_Upper = numeric(),
  stringsAsFactors = FALSE
)

# Loop through each pair and bootstrap all three metrics
for (pair in pairs) {
  item1 <- pair[1]; item2 <- pair[2]
  pair_name <- paste(item1, item2, sep = "_")
  
  # Cronbach's alpha bootstrap
  alpha_boot <- boot(data = merged_hexad, statistic = cronbach_alpha_fn,
                     R = 10000, item1 = item1, item2 = item2)
  alpha_est <- alpha_boot$t0
  alpha_ci  <- boot.ci(alpha_boot, type = "perc")$percent[4:5]
  
  # Spearman-Brown bootstrap
  sb_boot <- boot(data = merged_hexad, statistic = spearman_brown_fn,
                  R = 10000, item1 = item1, item2 = item2)
  sb_est <- sb_boot$t0
  sb_ci  <- boot.ci(sb_boot, type = "perc")$percent[4:5]
  
  # McDonald's omega bootstrap
  omega_boot <- boot(data = merged_hexad, statistic = mcdonald_omega_fn,
                     R = 10000, item1 = item1, item2 = item2)
  omega_est <- omega_boot$t0
  omega_ci  <- boot.ci(omega_boot, type = "perc")$percent[4:5]
  
  # Append to results
  results <- rbind(results, data.frame(
    Pair                    = pair_name,
    Alpha_Estimate          = round(alpha_est,  2),
    Alpha_CI_Lower          = round(alpha_ci[1], 2),
    Alpha_CI_Upper          = round(alpha_ci[2], 2),
    Spearman_Brown_Estimate = round(sb_est,     2),
    SB_CI_Lower             = round(sb_ci[1],  2),
    SB_CI_Upper             = round(sb_ci[2],  2),
    Omega_Estimate          = round(omega_est,  2),
    Omega_CI_Lower          = round(omega_ci[1], 2),
    Omega_CI_Upper          = round(omega_ci[2], 2)
  ))
}

# Print the results


kable(results, caption = "Internal reliability, Composite Reliability (CR) estimates, and Average Variance Extracted (AVE) for each Hexad-12 scale. ",digits  = 2)


```



## Correlaiton metric Kendals Tau
```{r}

# Reorganize columns in the desired order
columns <- c("philanthropist", "socializer", "achiever", "player", "disruptor", "freeSpirit")
data_subset <- data[columns]

# Initialize matrices for correlations and p-values
n <- length(columns)
cor_matrix <- matrix("", n, n, dimnames = list(columns, columns))
p_matrix <- matrix(NA, n, n, dimnames = list(columns, columns))

# Calculate Kendall Tau correlations and p-values
for (i in 1:n) {
  for (j in i:n) {
    if (i == j) {
      cor_matrix[i, j] <- "1.000"
    } else {
      test <- cor.test(data_subset[[i]], data_subset[[j]], method = "kendall", use = "pairwise.complete.obs")
      correlation <- round(test$estimate, 3)
      p_value <- round(test$p.value, 3)
      significance <- ifelse(p_value < 0.001, "***", 
                      ifelse(p_value < 0.01, "**", 
                      ifelse(p_value < 0.05, "*", "")))
      
      cor_matrix[i, j] <- ""
      cor_matrix[j, i] <- paste0(correlation, significance)
    }
  }
}

# Convert the correlation matrix to a data frame for display
cor_matrix_df <- as.data.frame(cor_matrix, row.names = columns)
rownames(cor_matrix_df) <- c("Philanthropic", "Socializer", "Achiever", "Player", "Disruptor", "Free Spirit")
colnames(cor_matrix_df) <- c("Philanthropic", "Socializer", "Achiever", "Player", "Disruptor", "Free Spirit")

# Display the correlation matrix as a nicely formatted RMarkdown table
cor_matrix_df %>%
  kbl(
    caption = "Lower Triangle of Kendall Tau Correlation Matrix with Significance. Significance Legend: *** p < 0.001, ** p < 0.01, * p < 0.05"
  ) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

# Item by Item Analysis
```{r}
# Required packages
library(psych)       # describe(), alpha(), omega()
library(lavaan)      # cfa()
library(semTools)    # omegaSem()
library(ggplot2)     # plotting
library(dplyr)       # data wrangling
library(purrr)       # map*

# Define your 12 item columns
item_cols <- c("A2","A4","D3","D4","F1","F3","P1","P4","R2","R4","S2","S4")
df_items  <- merged_hexad[ , item_cols ]

# 1. Descriptive statistics
desc_stats <- describe(df_items)

kable(desc_stats, caption = "Summary Statistics per item",digits  = 2)


# 2. Item–total correlations
merged_hexad$total_score <- rowSums(df_items, na.rm=TRUE)
item_total_corrs <- map_dbl(item_cols, ~ cor(
  merged_hexad[[.x]],
  merged_hexad$total_score - merged_hexad[[.x]],
  use = "pairwise.complete.obs"
))
names(item_total_corrs) <- item_cols



kable(item_total_corrs, caption = "Item–total correlation for each subscale",digits  = 2)



```



```{r}
# install.packages(c("ggplot2","diptest","tidyr","dplyr","purrr"))
library(ggplot2)
library(diptest)
library(tidyr)
library(dplyr)
library(purrr)

# 1. Define your 12 item columns and subset
item_cols <- c("A2","A4","D3","D4","F1","F3","P1","P4","R2","R4","S2","S4")
df_items <- merged_hexad[, item_cols]

# 2. Reshape to long format for plotting
df_long <- df_items %>%
  pivot_longer(cols      = everything(),
               names_to  = "item",
               values_to = "score")

# 3. Faceted histograms
ggplot(df_long, aes(x = score)) +
  geom_histogram(bins = 30, color = "black", fill = "lightblue") +
  facet_wrap(~ item, scales = "free") +
  labs(title = "Hexad-12 Item Distributions",
       x     = "Response",
       y     = "Count") +
  theme_minimal()


```

```{r}

#––– 1. Load libraries
library(moments)    # for skewness() and kurtosis()
library(multimode)  # for silverman.test()
library(dplyr)      # for data manipulation
library(tidyr)      # if you later want to reshape for plots
library(purrr)      # for map functions
library(ggplot2)    # for any plotting


for(it in item_cols) {
  x <- na.omit(merged_hexad[[it]])
  # Silverman’s test (mod0=1 ⇒ test unimodality vs. multimodality)
  si <- modetest(x, mod0 = 1, method = "SI", B = 10000)
  
  cat(sprintf(
    "%-3s  bw = %.3f  p = %.3f\n",
    it,
    si$statistic,   # critical bandwidth
    si$p.value      # p-value
  ))
}
```