---
title: "README"
output: 
  github_document:
    pandoc_args: ["--wrap=none"]
always_allow_html: true
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)

```

```{r}
# Load required libraries
# List of libraries to check, install (if missing), and load
libraries <- c(
  "knitr", "dplyr", "ggplot2", "tidyr", "multcomp", 
  "kableExtra", "FSA", "MVN", "lavaan", "semPlot", 
  "psych", "boot", "markdown"
)

# Function to check, install, and load libraries
check_and_load <- function(lib) {
  if (!require(lib, character.only = TRUE)) {
    install.packages(lib, dependencies = TRUE)
    library(lib, character.only = TRUE)
  }
}

# Apply the function to all libraries
lapply(libraries, check_and_load)

set.seed(123)  # For reproducibility


```


Download that from GitHub Repo: https://github.com/lopezbec/Hexad-12_Spanish_CHI2025

```{r}
# Define the GitHub raw CSV URL
github_url <- "https://raw.githubusercontent.com/lopezbec/Hexad-12_Spanish_CHI2025/refs/heads/main/Hexad_12_Spanish_data.csv"

# Define a local file path to save the downloaded file
local_file <- tempfile(fileext = ".csv")

# Download the file
download.file(github_url, local_file)

# Load the CSV into a dataframe
data <- read.csv(local_file,row.names = 1)

#remove 1st non-value colums


# Display the first few rows of the dataframe
head(data)
```
## General Statistics


#### Number of Participants
Each row represents a data point from a single participant. Total number of participants
```{r}
print(nrow(data))
```

#### Completion Time
```{r}
# Ensure Completion.Time is numeric
data$Completion.Time <- as.numeric(data$Completion.Time)

# Compute and create summary table
summary_table <- data.frame(
  Statistic = c("Average", "Median", "Min", "Max", "Standard Deviation", "Non-Empty Rows"),
  Value = c(
   round(mean(data$Completion.Time, na.rm = TRUE),2),
    round(median(data$Completion.Time, na.rm = TRUE),2),
    round(min(data$Completion.Time, na.rm = TRUE),2),
    round(max(data$Completion.Time, na.rm = TRUE),2),
    round(sd(data$Completion.Time, na.rm = TRUE),2),
    round(sum(!is.na(data$Completion.Time)),0)
  )
)

# Display the table
kable(summary_table, caption = "Summary Statistics for Completion Time")
```


#### Age
```{r}
# Ensure Completion.Time is numeric
data$Age <- as.numeric(data$Age)

# Compute and create summary table
summary_table <- data.frame(
  Statistic = c("Average", "Median", "Min", "Max", "Standard Deviation", "Non-Empty Rows"),
  Value = c(
   round(mean(data$Age, na.rm = TRUE),2),
    round(median(data$Age, na.rm = TRUE),2),
    round(min(data$Age, na.rm = TRUE),2),
    round(max(data$Age, na.rm = TRUE),2),
    round(sd(data$Age, na.rm = TRUE),2),
    round(sum(!is.na(data$Age)),0)
  )
)

# Display the table
kable(summary_table, caption = "Summary Statistics for Age")
```


#### Gender
```{r}
# Encode Gender as a factor with specific levels
gender_labels <- c("female", "male", "other")
data$Gender <- factor(data$Gender, levels = gender_labels)

# Compute summary statistics for Gender
gender_counts <- table(data$Gender)
total_count <- sum(gender_counts)
gender_percentages <- round((gender_counts / total_count) * 100, 2)

# Create a summary table
gender_summary_table <- data.frame(
  Gender = c(names(gender_counts), "Total"),
  Count = c(as.numeric(gender_counts), total_count),
  Percentage = c(paste0(gender_percentages, "%"), "100%")
)

# Display the table
kable(gender_summary_table, caption = "Summary of Gender Distribution with Totals")

```


#### Education Level
```{r}
# Encode Education Level as a factor with specific levels
educationlevel_labels <- c(
  "High School diploma",
  "Technical or vocational training",
  "Associate's degree",
  "Bachelor's degree",
  "Master's degree",
  "Professional degree",
  "Doctorate"
)

data$Education <- factor(data$Education, levels = educationlevel_labels)

# Compute counts and percentages
education_counts <- table(data$Education)
total_count <- sum(education_counts)
education_percentages <- round((education_counts / total_count) * 100, 2)

# Create a summary table
education_summary_table <- data.frame(
  EducationLevel = c(names(education_counts), "Total"),
  Count = c(as.numeric(education_counts), total_count),
  Percentage = c(paste0(education_percentages, "%"), "100%")
)

# Display the summary table
kable(education_summary_table, caption = "Summary of Education Level Distribution with Totals")
```


#### Playing Days
```{r}
# Ensure Completion.Time is numeric
data$Freq.Games <- as.numeric(data$Freq.Games)

# Compute and create summary table
summary_table <- data.frame(
  Statistic = c("Average", "Median", "Min", "Max", "Standard Deviation", "Non-Empty Rows"),
  Value = c(
   round(mean(data$Freq.Games, na.rm = TRUE),2),
    round(median(data$Freq.Games, na.rm = TRUE),2),
    round(min(data$Freq.Games, na.rm = TRUE),2),
    round(max(data$Freq.Games, na.rm = TRUE),2),
    round(sd(data$Freq.Games, na.rm = TRUE),2),
    round(sum(!is.na(data$Freq.Games)),0)
  )
)

# Display the table
kable(summary_table, caption = "Summary Statistics for Frequency of Play")
```

#### Country of Origin

```{r}
# Filter the data to exclude empty Country values
filtered_data <- data$Country[data$Country != ""]

# Compute counts and percentages
country_counts <- sort(table(filtered_data), decreasing = TRUE)
total_count <- sum(country_counts)
country_percentages <- round((country_counts / total_count) * 100, 2)

# Create a summary table with "Total" as the first row
summary_table <- data.frame(
  Country = c("Total", names(country_counts)),
  Count = c(total_count, as.numeric(country_counts)),
  Percentage = c("100%", paste0(country_percentages, "%"))
)

# Display the summary table
kable(summary_table, caption = "Summary of Country Distribution with Totals at the Top")

```



#### Hexad type  Distribution
```{r}
# Subset data to include the first six columns
subdata <- data[, c(1:6)]

# Specify the columns of interest
columns_of_interest <- c("Philanthropist", "Socializer", "Free Spirit", "Achiever", "Player", "Disruptor")

# Rename columns
colnames(subdata) <- columns_of_interest

# Calculate summary statistics and round to 2 decimal places
summary_stats <- data.frame(
  Column = columns_of_interest,
  Mean = round(sapply(subdata, mean, na.rm = TRUE), 2),
  Median = round(sapply(subdata, median, na.rm = TRUE), 2),
  Min = round(sapply(subdata, min, na.rm = TRUE), 2),
  Max = round(sapply(subdata, max, na.rm = TRUE), 2),
  SD = round(sapply(subdata, sd, na.rm = TRUE), 2)
)

summary_stats <- summary_stats[order(-summary_stats$Mean), -1]

# Display the summary statistics table
kable(summary_stats, caption = "Summary Statistics for Hexad Types")


```


## Hexad Player type Distributions Analysis By Country (top 10)

#### Distribution by Country (Top 10)

```{r}
#Shorten Countries Name for easy of tables
data[data$Country=="Bolivia (Estado Plurinacional de)", 'Country' ]<-"Bolivia"
data[data$Country=="Venezuela (República Bolivariana de)", 'Country' ]<-"Venezuela"
data[data$Country==" Estados Unidos de América", 'Country' ]<-"USA"
data$Country<-as.factor(data$Country)

# Filter out rows where 'Country' is missing
hexad_data_clean <- data %>% filter(Country != "")

# Identify the top 5, top 10, and top 20 most common countries
top_countries <- hexad_data_clean %>%
  count(Country, sort = TRUE)

top_10_countries <- top_countries %>% slice(1:10) %>% pull(Country)

# Filter data for these top countries
hexad_top10 <- hexad_data_clean %>% filter(Country %in% top_10_countries)


```

```{r}


# Calculate and round summary statistics by Country for each player type
hexad_summary <- hexad_top10 %>%
  group_by(Country) %>%
  summarise(
    sample_size = n(),
    philanthropist_mean = round(mean(philanthropist, na.rm = TRUE), 2),
    philanthropist_median = round(median(philanthropist, na.rm = TRUE), 2),
    philanthropist_min = round(min(philanthropist, na.rm = TRUE), 2),
    philanthropist_max = round(max(philanthropist, na.rm = TRUE), 2),
    philanthropist_sd = round(sd(philanthropist, na.rm = TRUE), 2),
    
    socializer_mean = round(mean(socializer, na.rm = TRUE), 2),
    socializer_median = round(median(socializer, na.rm = TRUE), 2),
    socializer_min = round(min(socializer, na.rm = TRUE), 2),
    socializer_max = round(max(socializer, na.rm = TRUE), 2),
    socializer_sd = round(sd(socializer, na.rm = TRUE), 2),
    
    freeSpirit_mean = round(mean(freeSpirit, na.rm = TRUE), 2),
    freeSpirit_median = round(median(freeSpirit, na.rm = TRUE), 2),
    freeSpirit_min = round(min(freeSpirit, na.rm = TRUE), 2),
    freeSpirit_max = round(max(freeSpirit, na.rm = TRUE), 2),
    freeSpirit_sd = round(sd(freeSpirit, na.rm = TRUE), 2),
    
    achiever_mean = round(mean(achiever, na.rm = TRUE), 2),
    achiever_median = round(median(achiever, na.rm = TRUE), 2),
    achiever_min = round(min(achiever, na.rm = TRUE), 2),
    achiever_max = round(max(achiever, na.rm = TRUE), 2),
    achiever_sd = round(sd(achiever, na.rm = TRUE), 2),
    
    player_mean = round(mean(player, na.rm = TRUE), 2),
    player_median = round(median(player, na.rm = TRUE), 2),
    player_min = round(min(player, na.rm = TRUE), 2),
    player_max = round(max(player, na.rm = TRUE), 2),
    player_sd = round(sd(player, na.rm = TRUE), 2),
    
    disruptor_mean = round(mean(disruptor, na.rm = TRUE), 2),
    disruptor_median = round(median(disruptor, na.rm = TRUE), 2),
    disruptor_min = round(min(disruptor, na.rm = TRUE), 2),
    disruptor_max = round(max(disruptor, na.rm = TRUE), 2),
    disruptor_sd = round(sd(disruptor, na.rm = TRUE), 2)
  ) %>%
  arrange(desc(sample_size)) # Sort by sample size

# Transpose the table
hexad_summary_t <- hexad_summary %>%
  pivot_longer(cols = -Country, names_to = "Statistic", values_to = "Value") %>%
  pivot_wider(names_from = Country, values_from = Value)

# Generate an RMarkdown table
hexad_summary_t %>%
  kbl(caption = "Transposed Summary Statistics for Hexad Player Types by Country") %>%
  kable_styling(full_width = TRUE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```



#### Normality tests
```{r}


# Initialize an empty data frame to store the results
shapiro_results <- data.frame(
  Row = c("Philanthropist", "Socializer", "Free Spirit", "Achiever", "Player", "Disruptor"),
  Shapiro_Wilk = numeric(6),
  P_value = numeric(6),
  stringsAsFactors = FALSE
)

# List of player types
player_types <- c("philanthropist", "socializer", "freeSpirit", "achiever", "player", "disruptor")

# Loop through each player type and perform the Shapiro-Wilk test
for (i in seq_along(player_types)) {
  type <- player_types[i]
  
  # Perform ANOVA to get residuals
  anova_5 <- aov(as.formula(paste(type, "~ Country")), data = hexad_top10)
  
  # Perform Shapiro-Wilk Test
  shapiro_test <- shapiro.test(residuals(anova_5))
  
  # Save results
  shapiro_results$Shapiro_Wilk[i] <- round(shapiro_test$statistic, 2)
  shapiro_results$P_value[i] <- shapiro_test$p.value
}

# Rename columns to match desired output
colnames(shapiro_results) <- c("Row", "Shapiro-Wilk", "p-value")

# Create and display the RMarkdown table
shapiro_results %>%
  kbl(caption = "Shapiro-Wilk Normality Test for Player Types by Country") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```

#### Non-parametric ANOVA tests
```{r}

# Initialize an empty data frame to store the results
results <- data.frame(
  Row = c("Philanthropist", "Socializer", "Free Spirit", "Achiever", "Player", "Disruptor"),
  Kruskal_Wallis_Chi_Sq = numeric(6),
  Degrees_of_Freedom = numeric(6),
  p_value = numeric(6),
  stringsAsFactors = FALSE
)

# List of player types
player_types <- c("philanthropist", "socializer", "freeSpirit", "achiever", "player", "disruptor")

# Loop through each player type and perform the Kruskal-Wallis test
for (i in seq_along(player_types)) {
  type <- player_types[i]
  
  # Perform Kruskal-Wallis Test
  kw_test <- kruskal.test(as.formula(paste(type, "~ Country")), data = hexad_top10)
  
  # Save results
  results$Kruskal_Wallis_Chi_Sq[i] <- round(kw_test$statistic, 2)
  results$Degrees_of_Freedom[i] <- kw_test$parameter
  results$p_value[i] <- round(kw_test$p.value, 3)
}

# Rename columns to match desired output
colnames(results) <- c(
  "Row",
  "Kruskal-Wallis χ^2",
  "Degrees of Freedom",
  "p-value"
)

# Create and display the RMarkdown table
results %>%
  kbl(caption = "Kruskal-Wallis Test Results for Player Types by Country") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```

#### Dunn test for multiple comparison for Achiever

```{r}

# Perform Dunn Test with Bonferroni adjustment
posthoc_test <- dunnTest(achiever ~ Country, data = hexad_top10, method = "bonferroni")

# Extract and round results
posthoc_results <- posthoc_test$res %>%
  mutate(across(where(is.numeric), ~ round(.x, 3))) # Round all numeric columns to 3 decimal places

# Format results as a nice RMarkdown table
posthoc_results %>%
  kbl(caption = "Post Hoc Dunn Test Results for Achiever by Country (Bonferroni Adjustment)") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```


## Confirmatory Factor Analysis

#### Mardia’s multivariate normality test 
```{r}

# Perform Mardia's test for multivariate normality
mardia_test <- mvn(data = data[, 7:18], mvnTest = "mardia")
mardia_test_N<-mardia_test$multivariateNormality
# Extract and prepare the results for the table
mardia_results <- data.frame(
  Test = c("Skewness", "Kurtosis"),
  Statistic = c(
    (mardia_test_N[1,2]),
    (mardia_test_N[2,2])
    
  ),
  P_Value = c(
  (mardia_test_N[1,3]),
    (mardia_test_N[2,3])
    
  )
)

# Create and display the RMarkdown table
mardia_results %>%
  kbl(caption = "Mardia's Test for Multivariate Normality") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

#### Weighted Least Squares Mean and Variance Adjusted   CFA
```{r}
# Specify the CFA model
cfa_model <- '
  Achiever =~ A2 + A4
  Disruptor =~ D3 + D4
  FreeSpirit =~ F1 + F3
  Philanthropist =~ P1 + P4
  Player =~ R2 + R4
  Socializer =~ S2 + S4
'

# Fit the CFA model
fit <- cfa(cfa_model, data = data,std.lv=TRUE, auto.efa = TRUE, estimator = "WLSMV")
 #
# Global Fit Metrics
results<-summary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)


results$pe[1:12, c(1:8,10)]


```



###  CFA  metrics with confident interval
```{r}

reliability_metrics <- semTools::reliability(fit)
CR<-reliability_metrics[2,]
AVE<-reliability_metrics[5,]
  

  # Extract relevant fit measures
  fit_measures_final <- fitMeasures(fit,standardized = TRUE, rsquare = TRUE, c("chisq.scaled", "srmr", "tli.scaled", "cfi.scaled", "rmsea.scaled"))

  fit_measures_final<-c(fit_measures_final, CR, AVE)
  

# Function to take a bootstrap sample and perform CFA, returning CR, Omega, and AVE
bootstrap_cfa_cr_omega_ave <- function(data, cfa_model) {
  # Generate a bootstrap sample (with replacement)
  bootstrap_sample <- data[sample(1:nrow(data), replace = TRUE), ]
  

# Fit the CFA model
fit <- cfa(cfa_model, data = bootstrap_sample,std.lv=TRUE, auto.efa = TRUE, estimator = "WLSMV")

 # Calculate reliability metrics (including Composite Reliability)
reliability_metrics <- semTools::reliability(fit)
  CR<-reliability_metrics[2,]
 AVE<-reliability_metrics[5,]
  

  # Extract relevant fit measures
  fit_measures <- fitMeasures(fit, c("chisq.scaled", "srmr", "tli.scaled", "cfi.scaled", "rmsea.scaled"))

  fit_measures<-c(fit_measures, CR, AVE)
 
  return(fit_measures)
}

# Perform bootstrap sampling
set.seed(123)  # For reproducibility
num_bootstrap <- 10  # Number of bootstrap samples8u-p0

# Initialize matrix to store CR, Omega, and AVE
fit_metrics <- matrix(NA, nrow = num_bootstrap, ncol = 17)
# List of columns to calculate confidence intervals for
columns_to_ci <- c("chisq.scaled", "srmr", "tli.scaled", "cfi.scaled", "rmsea.scaled", 
                   "CR.Achiever", "CR.Disruptor", "CR.FreeSpirit", "CR.Philanthropist", 
                   "CR.Player", "CR.Socializer", 
                   "AVE.Achiever", "AVE.Disruptor", "AVE.FreeSpirit", 
                   "AVE.Philanthropist", "AVE.Player", "AVE.Socializer")
colnames(fit_metrics) <- columns_to_ci

# Bootstrap loop
for (i in 1:num_bootstrap) {
  fit_metrics[i, ] <- bootstrap_cfa_cr_omega_ave(data, cfa_model)
}


  names(fit_measures_final) <- columns_to_ci
  #summary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)
  

# Initialize an empty data frame to store the results
results_table <- data.frame(
  Metric = columns_to_ci,
  Estimate = numeric(length(columns_to_ci)),
  CI_Lower = numeric(length(columns_to_ci)),
  CI_Upper = numeric(length(columns_to_ci)),
  stringsAsFactors = FALSE
)

# Populate the results table with confidence intervals and estimates
for (i in seq_along(columns_to_ci)) {
  col <- columns_to_ci[i]
  ci <- quantile(fit_metrics[, col], probs = c(0.025, 0.975))
  results_table$Estimate[i] <- round(fit_measures_final[col], 3)  # Add the estimate and round
  results_table$CI_Lower[i] <- round(ci[1], 3)  # Round lower CI
  results_table$CI_Upper[i] <- round(ci[2], 3)  # Round upper CI
}

# Correctly print the table in a nicely formatted way
results_table %>%
  kbl(caption = "CFA  metrics with confident interval", format = "markdown") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

#### CFA Standardized residuals
```{r}
# Local Fit: Residuals  Standardized residuals
standardized_residuals <- residuals(fit, type = "standardized")
print(standardized_residuals)
```

#### CFA Correlation residuals
```{r}
#Correlation residuals
residuals(fit, type = "cor")
```


## Internal Reliability

```{r}


# Define pairs of columns for which to calculate Cronbach's alpha and Spearman-Brown reliability
pairs <- list(
  c("P1", "P4"),
   c("S2", "S4"),
  c("A2", "A4"),
    c("R2", "R4"),
  c("D3", "D4"),
   c("F1", "F3")
   )



num_bootstrap <- 10
# Define function for Cronbach's alpha calculation
cronbach_alpha_fn <- function(data, indices, item1, item2) {
  sample_data <- data[indices, c(item1, item2)]
  alpha <- psych::alpha(sample_data)$total$raw_alpha
  return(alpha)
}

# Define function for Spearman-Brown calculation
spearman_brown_fn <- function(data, indices, item1, item2) {
  sample_data <- data[indices, ]
  correlation <- cor(sample_data[[item1]], sample_data[[item2]], method = "pearson")
  spearman_brown <- 2 * correlation / (1 + correlation)
  return(spearman_brown)
}

# Initialize a data frame to store results
results <- data.frame(Pair = character(), Alpha_Estimate = numeric(), Alpha_CI_Lower = numeric(), Alpha_CI_Upper = numeric(),
                      Spearman_Brown_Estimate = numeric(), SB_CI_Lower = numeric(), SB_CI_Upper = numeric())

# Loop through each pair and calculate Cronbach's alpha and Spearman-Brown with confidence intervals
for (pair in pairs) {
  item1 <- pair[1]
  item2 <- pair[2]
  
  # Bootstrapping for Cronbach's alpha
  alpha_boot <- boot(data = data, statistic = cronbach_alpha_fn, R = num_bootstrap, item1 = item1, item2 = item2)
  alpha_estimate <- alpha_boot$t0
  alpha_ci <- boot.ci(alpha_boot, type = "perc")$percent[4:5]
  
  # Bootstrapping for Spearman-Brown
  sb_boot <- boot(data = data, statistic = spearman_brown_fn, R = num_bootstrap, item1 = item1, item2 = item2)
  sb_estimate <- sb_boot$t0
  sb_ci <- boot.ci(sb_boot, type = "perc")$percent[4:5]
  
  # Append results to the data frame
  results <- rbind(results, data.frame(
    Pair = paste(item1, item2, sep = "_"),
    Alpha_Estimate = round(alpha_estimate,2),
    Alpha_CI_Lower = round(alpha_ci[1],2),
    Alpha_CI_Upper = round(alpha_ci[2],2),
    Spearman_Brown_Estimate = round(sb_estimate,2),
    SB_CI_Lower = round(sb_ci[1],2),
    SB_CI_Upper = round(sb_ci[2],2)
  ))
}

# Print the results
print(results)
# Define row names
row_names <- c("Philanthropist", "Socializer", "Achiever", "Player", "Disruptor", "Free Spirit")

# Add row names to the results data frame
results <- cbind(Row = row_names, results)

# Display the results in an RMarkdown-friendly table
results %>%
  kbl(caption = "Reliability Analysis: Cronbach's Alpha and Spearman-Brown Results") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```
## Correlaiton matric Kendals Tau
```{r}
library(dplyr)
library(knitr)
library(kableExtra)

# Reorganize columns in the desired order
columns <- c("philanthropist", "socializer", "achiever", "player", "disruptor", "freeSpirit")
data_subset <- data[columns]

# Initialize matrices for correlations and p-values
n <- length(columns)
cor_matrix <- matrix("", n, n, dimnames = list(columns, columns))
p_matrix <- matrix(NA, n, n, dimnames = list(columns, columns))

# Calculate Kendall Tau correlations and p-values
for (i in 1:n) {
  for (j in i:n) {
    if (i == j) {
      cor_matrix[i, j] <- "1.000"
    } else {
      test <- cor.test(data_subset[[i]], data_subset[[j]], method = "kendall", use = "pairwise.complete.obs")
      correlation <- round(test$estimate, 3)
      p_value <- round(test$p.value, 3)
      significance <- ifelse(p_value < 0.001, "***", 
                      ifelse(p_value < 0.01, "**", 
                      ifelse(p_value < 0.05, "*", "")))
      
      cor_matrix[i, j] <- ""
      cor_matrix[j, i] <- paste0(correlation, significance)
    }
  }
}

# Convert the correlation matrix to a data frame for display
cor_matrix_df <- as.data.frame(cor_matrix, row.names = columns)
rownames(cor_matrix_df) <- c("Philanthropic", "Socializer", "Achiever", "Player", "Disruptor", "Free Spirit")
colnames(cor_matrix_df) <- c("Philanthropic", "Socializer", "Achiever", "Player", "Disruptor", "Free Spirit")

# Display the correlation matrix as a nicely formatted RMarkdown table
cor_matrix_df %>%
  kbl(
    caption = "Lower Triangle of Kendall Tau Correlation Matrix with Significance. Significance Legend: *** p < 0.001, ** p < 0.01, * p < 0.05"
  ) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

